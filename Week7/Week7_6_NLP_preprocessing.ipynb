{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#c3b235\">NLP text preprocessing\n",
    "\n",
    "- __In NLP text preprocessing is more essential, and NLP is an art to extract some information from the text documents.__\n",
    "- ___First step is the preprocessing, and how to process the data are given in detail___\n",
    "\n",
    "\n",
    "\n",
    "- __Import Libraries and download NLTK tools__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jayanthikishore/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jayanthikishore/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jayanthikishore/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jayanthikishore/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from termcolor import colored\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stop')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('all')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#ea0ea1\">Text datasets reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata1 = pd.read_csv(\"~/Downloads/ML_classwork/Week7_srrt/tweets_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>*screams in 25 different languages*</th>\n",
       "      <th>0.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alton brown just did a livestream and he burne...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>@Trollkrattos Juan Carlos Salvador The Secret ...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>@devon_breneman hopefully it doesn't electrocu...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Businesses are deluged with invokces. Make you...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>#BREAKING411 4 police officers arrested for ab...</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>@News@ Refugio oil spill may have been costlie...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1863 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    *screams in 25 different languages*  0.6\n",
       "0     Families to sue over Legionnaires: More than 4...  0.1\n",
       "1     Pandemonium In Aba As Woman Delivers Baby With...  0.4\n",
       "2     My emotions are a train wreck. My body is a tr...  0.2\n",
       "3     Alton brown just did a livestream and he burne...  0.5\n",
       "4     @TinyJecht Are you another Stand-user? If you ...  0.5\n",
       "...                                                 ...  ...\n",
       "1858  @Trollkrattos Juan Carlos Salvador The Secret ...  0.5\n",
       "1859  @devon_breneman hopefully it doesn't electrocu...  0.5\n",
       "1860  Businesses are deluged with invokces. Make you...  0.5\n",
       "1861  #BREAKING411 4 police officers arrested for ab...  0.1\n",
       "1862  @News@ Refugio oil spill may have been costlie...  0.2\n",
       "\n",
       "[1863 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#830eea\">Text dataset information and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1863 entries, 0 to 1862\n",
      "Data columns (total 2 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   *screams in 25 different languages*  1863 non-null   object \n",
      " 1   0.6                                  1863 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 29.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Total information of the data file\n",
    "tdata1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data shape\n",
    "tdata1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#330eea\">Rename column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Families to sue over Legionnaires: More than 4...\n",
       "1       Pandemonium In Aba As Woman Delivers Baby With...\n",
       "2       My emotions are a train wreck. My body is a tr...\n",
       "3       Alton brown just did a livestream and he burne...\n",
       "4       @TinyJecht Are you another Stand-user? If you ...\n",
       "                              ...                        \n",
       "1858    @Trollkrattos Juan Carlos Salvador The Secret ...\n",
       "1859    @devon_breneman hopefully it doesn't electrocu...\n",
       "1860    Businesses are deluged with invokces. Make you...\n",
       "1861    #BREAKING411 4 police officers arrested for ab...\n",
       "1862    @News@ Refugio oil spill may have been costlie...\n",
       "Name: tweet, Length: 1863, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renamming the column names for the text dataset\n",
    "tdata = tdata1.rename(columns={\"*screams in 25 different languages*\":\"tweet\", \"0.6\":\"lbel\"})\n",
    "tdata.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0e4cea\">Display the first document from the text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the first document\n",
    "tdata.tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0e4cea\">Binning the y column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>lbel</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alton brown just did a livestream and he burne...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  lbel  label\n",
       "0  Families to sue over Legionnaires: More than 4...   0.1      1\n",
       "1  Pandemonium In Aba As Woman Delivers Baby With...   0.4      2\n",
       "2  My emotions are a train wreck. My body is a tr...   0.2      1\n",
       "3  Alton brown just did a livestream and he burne...   0.5      3\n",
       "4  @TinyJecht Are you another Stand-user? If you ...   0.5      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binning the y values to integers\n",
    "bins = np.array([0.0,0.25,0.5,0.75])\n",
    "# bins = np.array([0.0,0.2,0.4,0.6,0.8])\n",
    "tdata['label'] = np.digitize(tdata['lbel'],bins)\n",
    "# tdata['label'] = np.digitize(tdata['lble'],bins=[0.5])\n",
    "tdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext = tdata.tweet\n",
    "ydata = tdata.label\n",
    "ptext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB\n"
     ]
    }
   ],
   "source": [
    "print(ptext[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0eead0\">Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# temp = []\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "def preprocess(data):\n",
    "    temp = []\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()     #converting lowercase\n",
    "\n",
    "        #Removal of HTTP links/URLs mixed up in any text:\n",
    "        sentence = re.sub('http://\\S+|https://\\S+', '', sentence)\n",
    "        #OR \n",
    "        #sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "        #sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',sentence, flags=re.MULTILINE) #remove the URLs\n",
    "\n",
    "        #punctuations:'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        sentence = \"\".join([char for char in sentence if char not in string.punctuation])\n",
    "        sentence = re.sub(r'\\d+', '', sentence)    # removing numbers\n",
    "        sentence = sentence.strip()        # removing extra white spaces\n",
    "        tokens = nltk.word_tokenize(sentence)[2 :]\n",
    "    #     tokens = nltk.word_tokenize(sentence)\n",
    "        sentence = [l.lower() for l in tokens]\n",
    "        filtered_result = list(filter(lambda l: l not in stop_words, sentence))\n",
    "        lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]    \n",
    "\n",
    "        #(or)\n",
    "    #     sentence = re.sub(r'[?|!|\\'|\"|#!$%&()*+-@]',r'',sentence)\n",
    "    #     sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "        \n",
    "        temp.append(lemmas) \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xdata_cleaned = preprocess(ptext)\n",
    "# Xtest_cleaned = preprocess(Xtest)\n",
    "ptext_cleaned = preprocess(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sue',\n",
       "  'legionnaire',\n",
       "  'family',\n",
       "  'affected',\n",
       "  'fatal',\n",
       "  'outbreak',\n",
       "  'legionnaire',\n",
       "  'disea'],\n",
       " ['aba', 'woman', 'delivers', 'baby', 'without', 'face', 'photo']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(Xdata_cleaned[0])\n",
    "ptext_cleaned[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized or cleaning dataset adding into original datafile for chcking\n",
    "tdata['Normalized_tweet'] = ptext_cleaned\n",
    "tdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0eeaa4\">Original text and cleaned/normalized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>[sue, legionnaire, family, affected, fatal, ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
       "      <td>[aba, woman, delivers, baby, without, face, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>[train, wreck, body, train, wreck, im, wreck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alton brown just did a livestream and he burne...</td>\n",
       "      <td>[livestream, burned, butter, touched, hot, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>[another, standuser, detonate, killer, queen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  Families to sue over Legionnaires: More than 4...   \n",
       "1  Pandemonium In Aba As Woman Delivers Baby With...   \n",
       "2  My emotions are a train wreck. My body is a tr...   \n",
       "3  Alton brown just did a livestream and he burne...   \n",
       "4  @TinyJecht Are you another Stand-user? If you ...   \n",
       "\n",
       "                                    Normalized_tweet  \n",
       "0  [sue, legionnaire, family, affected, fatal, ou...  \n",
       "1  [aba, woman, delivers, baby, without, face, ph...  \n",
       "2      [train, wreck, body, train, wreck, im, wreck]  \n",
       "3  [livestream, burned, butter, touched, hot, pla...  \n",
       "4      [another, standuser, detonate, killer, queen]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata[['tweet','Normalized_tweet']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0eea7c\">Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sue': 5,\n",
       " 'legionnaire': 12,\n",
       " 'family': 24,\n",
       " 'affected': 7,\n",
       " 'fatal': 11,\n",
       " 'outbreak': 6,\n",
       " 'disea': 5,\n",
       " 'aba': 3,\n",
       " 'woman': 11,\n",
       " 'delivers': 2,\n",
       " 'baby': 7,\n",
       " 'without': 15,\n",
       " 'face': 14,\n",
       " 'photo': 11,\n",
       " 'train': 26,\n",
       " 'wreck': 14,\n",
       " 'body': 34,\n",
       " 'im': 47,\n",
       " 'livestream': 1,\n",
       " 'burned': 9,\n",
       " 'butter': 4,\n",
       " 'touched': 1,\n",
       " 'hot': 9,\n",
       " 'plate': 2,\n",
       " 'soon': 2,\n",
       " 'made': 10,\n",
       " 'nut': 1,\n",
       " 'joke': 3,\n",
       " 'another': 12,\n",
       " 'standuser': 2,\n",
       " 'detonate': 9,\n",
       " 'killer': 3,\n",
       " 'queen': 3,\n",
       " 'timed': 1,\n",
       " 'concert': 2,\n",
       " 'screamed': 11,\n",
       " 'minute': 17,\n",
       " 'straight': 2,\n",
       " 'florida': 3,\n",
       " 'forest': 18,\n",
       " 'service': 19,\n",
       " 'firefighter': 2,\n",
       " 'could': 17,\n",
       " 'deployed': 1,\n",
       " 'california': 24,\n",
       " 'help': 13,\n",
       " 'contain': 2,\n",
       " 'fire': 73,\n",
       " 'detail': 1,\n",
       " 'atomic': 9,\n",
       " 'bomb': 22,\n",
       " 'japan': 8,\n",
       " 'still': 32,\n",
       " 'struggle': 3,\n",
       " 'war': 20,\n",
       " 'past': 6,\n",
       " 'anniversary': 6,\n",
       " 'devastation': 9,\n",
       " 'wrought': 4,\n",
       " 'b': 6,\n",
       " 'killed': 20,\n",
       " 'civilian': 3,\n",
       " 'ground': 4,\n",
       " 'jet': 3,\n",
       " 'first': 24,\n",
       " 'bombed': 10,\n",
       " 'city': 13,\n",
       " 'main': 3,\n",
       " 'street': 8,\n",
       " 'dramatically': 1,\n",
       " 'plummeted': 1,\n",
       " 'burn': 4,\n",
       " 'whole': 11,\n",
       " 'gotham': 1,\n",
       " 'bcs': 1,\n",
       " 'gon': 10,\n",
       " 'na': 16,\n",
       " 'laugh': 2,\n",
       " 'everyone': 15,\n",
       " 'theyre': 8,\n",
       " 'panicking': 7,\n",
       " 'phone': 8,\n",
       " 'worstoverdose': 1,\n",
       " 'scream': 9,\n",
       " 'jaileens': 1,\n",
       " 'caked': 1,\n",
       " 'look': 29,\n",
       " 'email': 2,\n",
       " 'people': 44,\n",
       " 'use': 7,\n",
       " 'report': 13,\n",
       " 'sthing': 1,\n",
       " 'think': 18,\n",
       " 'hazarddangerous': 1,\n",
       " 'old': 16,\n",
       " 'lady': 7,\n",
       " 'went': 15,\n",
       " 'something': 9,\n",
       " 'ask': 1,\n",
       " 'alone': 4,\n",
       " 'coming': 5,\n",
       " 'tsunami': 9,\n",
       " 'showing': 1,\n",
       " 'storm': 16,\n",
       " 'ne': 2,\n",
       " 'edmond': 1,\n",
       " 'severe': 8,\n",
       " 'hail': 11,\n",
       " 'mph': 14,\n",
       " 'wind': 16,\n",
       " 'possible': 8,\n",
       " 'okwx': 3,\n",
       " 'attack': 22,\n",
       " 'gun': 8,\n",
       " 'grabber': 2,\n",
       " 'demand': 2,\n",
       " 'life': 39,\n",
       " 'tragedy': 6,\n",
       " 'monicas': 1,\n",
       " 'hair': 3,\n",
       " 'given': 2,\n",
       " 'season': 4,\n",
       " 'friend': 10,\n",
       " 'case': 7,\n",
       " 'drowning': 4,\n",
       " 'sweatfyi': 1,\n",
       " 'mile': 4,\n",
       " 'east': 4,\n",
       " 'pickens': 1,\n",
       " 'moving': 8,\n",
       " 'pea': 1,\n",
       " 'size': 1,\n",
       " 'gust': 4,\n",
       " 'scwx': 1,\n",
       " 'dude': 5,\n",
       " 'obliterated': 5,\n",
       " 'meek': 2,\n",
       " 'mill': 2,\n",
       " 'dont': 31,\n",
       " 'judge': 1,\n",
       " 'book': 11,\n",
       " 'cover': 5,\n",
       " 'explode': 7,\n",
       " 'sewage': 1,\n",
       " 'open': 7,\n",
       " 'housing': 2,\n",
       " 'estate': 1,\n",
       " 'irish': 4,\n",
       " 'independent': 2,\n",
       " 'aug': 6,\n",
       " 'team': 9,\n",
       " 'usagi': 1,\n",
       " 'even': 26,\n",
       " 'blew': 9,\n",
       " 'entire': 8,\n",
       " 'solar': 2,\n",
       " 'system': 4,\n",
       " 'airhead': 1,\n",
       " 'misstep': 1,\n",
       " 'good': 24,\n",
       " 'chunk': 1,\n",
       " 'running': 4,\n",
       " 'time': 37,\n",
       " 'trapped': 8,\n",
       " 'effective': 3,\n",
       " 'claustrophobic': 1,\n",
       " 'thriller': 3,\n",
       " 'try': 9,\n",
       " 'always': 17,\n",
       " 'end': 11,\n",
       " 'sinking': 14,\n",
       " 'way': 28,\n",
       " 'feel': 20,\n",
       " 'drank': 1,\n",
       " 'vodka': 1,\n",
       " 'ice': 2,\n",
       " 'would': 30,\n",
       " 'bag': 26,\n",
       " 'may': 24,\n",
       " 'logically': 1,\n",
       " 'right': 19,\n",
       " 'call': 6,\n",
       " 'maybe': 4,\n",
       " 'act': 8,\n",
       " 'mass': 17,\n",
       " 'murder': 8,\n",
       " 'cant': 15,\n",
       " 'sanction': 2,\n",
       " 'delivered': 1,\n",
       " 'big': 13,\n",
       " 'screen': 10,\n",
       " 'safe': 3,\n",
       " 'sound': 12,\n",
       " 'like': 87,\n",
       " 'yearold': 5,\n",
       " 'superstar': 1,\n",
       " 'girl': 16,\n",
       " 'travel': 4,\n",
       " 'fame': 1,\n",
       " 'freeway': 2,\n",
       " 'weakening': 1,\n",
       " 'move': 4,\n",
       " 'se': 4,\n",
       " 'towards': 2,\n",
       " 'lubbock': 1,\n",
       " 'area': 12,\n",
       " 'outflow': 1,\n",
       " 'boundary': 2,\n",
       " 'create': 2,\n",
       " 'dust': 8,\n",
       " 'quarantine': 7,\n",
       " 'offensive\\x89Ã»': 1,\n",
       " 'onlinecommunities': 1,\n",
       " 'reddit': 1,\n",
       " 'amageddon': 1,\n",
       " 'freespeech': 1,\n",
       " 'day': 48,\n",
       " 'disaster': 26,\n",
       " 'emotion': 2,\n",
       " 'happening': 4,\n",
       " 'lie': 5,\n",
       " 'drift': 1,\n",
       " 'away': 10,\n",
       " 'storming': 1,\n",
       " 'around': 10,\n",
       " 'little': 9,\n",
       " 'bebacksoon': 1,\n",
       " 'four': 4,\n",
       " 'year': 32,\n",
       " 'making': 10,\n",
       " 'home': 25,\n",
       " 'razed': 7,\n",
       " 'northern': 14,\n",
       " 'wildfire': 22,\n",
       " 'abc': 7,\n",
       " 'news': 27,\n",
       " 'second': 4,\n",
       " 'perfect': 4,\n",
       " 'heart': 17,\n",
       " 'place': 11,\n",
       " 'innocent': 3,\n",
       " 'wellmeaning': 1,\n",
       " 'wrecked': 7,\n",
       " 'car': 21,\n",
       " 'costing': 1,\n",
       " 'apiece': 1,\n",
       " 'purchased': 1,\n",
       " 'film': 36,\n",
       " 'anybody': 2,\n",
       " 'else': 9,\n",
       " 'problem': 3,\n",
       " 'circle': 2,\n",
       " 'epicentre': 2,\n",
       " 'ride': 4,\n",
       " 'quiet': 2,\n",
       " 'cadence': 1,\n",
       " 'pure': 2,\n",
       " 'finesse': 1,\n",
       " 'far': 7,\n",
       " 'shortage': 1,\n",
       " 'dilutes': 1,\n",
       " 'potency': 1,\n",
       " 'otherwise': 2,\n",
       " 'respectable': 1,\n",
       " 'action': 8,\n",
       " 'bride': 3,\n",
       " 'halfhour': 1,\n",
       " 'long': 12,\n",
       " 'come': 14,\n",
       " 'replete': 2,\n",
       " 'flattering': 1,\n",
       " 'sense': 5,\n",
       " 'mystery': 4,\n",
       " 'quietness': 1,\n",
       " 'birthday': 2,\n",
       " 'bruh': 2,\n",
       " 'windstorm': 9,\n",
       " 'sheer': 1,\n",
       " 'recovery': 2,\n",
       " 'update': 8,\n",
       " 'leelanau': 1,\n",
       " 'amp': 58,\n",
       " 'grand': 1,\n",
       " 'traverse': 2,\n",
       " 'state': 13,\n",
       " 'emergency': 22,\n",
       " 'extended': 1,\n",
       " 'besafe': 1,\n",
       " 'imax': 2,\n",
       " 'strap': 1,\n",
       " 'pair': 2,\n",
       " 'goggles': 1,\n",
       " 'shut': 1,\n",
       " 'real': 9,\n",
       " 'world': 31,\n",
       " 'take': 28,\n",
       " 'vicarious': 1,\n",
       " 'voyage': 1,\n",
       " 'last': 20,\n",
       " 'frontier': 1,\n",
       " 'space': 4,\n",
       " 'airport': 6,\n",
       " 'get': 61,\n",
       " 'swallowed': 6,\n",
       " 'sandstorm': 9,\n",
       " 'though': 7,\n",
       " 'refreshingly': 2,\n",
       " 'novel': 4,\n",
       " 'don\\x89Ã»Âªt': 1,\n",
       " 'want': 17,\n",
       " 'mention': 3,\n",
       " 'let\\x89Ã»Âªs': 1,\n",
       " 'anything': 5,\n",
       " 'lead': 4,\n",
       " 'best': 27,\n",
       " 'american': 9,\n",
       " 'movie': 31,\n",
       " 'troubled': 1,\n",
       " 'teen': 2,\n",
       " 'since': 14,\n",
       " 'whatever': 1,\n",
       " 'win': 8,\n",
       " 'kerry': 1,\n",
       " 'obliteration': 7,\n",
       " 'profit': 2,\n",
       " 'rise': 7,\n",
       " 'billion': 1,\n",
       " 'worst': 4,\n",
       " 'natural': 11,\n",
       " 'claim': 7,\n",
       " 'language': 4,\n",
       " 'story': 25,\n",
       " 'lively': 1,\n",
       " 'script': 2,\n",
       " 'sharp': 1,\n",
       " 'acting': 3,\n",
       " 'partially': 1,\n",
       " 'animated': 1,\n",
       " 'interlude': 1,\n",
       " 'make': 34,\n",
       " 'kiss': 2,\n",
       " 'seem': 1,\n",
       " 'minty': 1,\n",
       " 'fresh': 3,\n",
       " 'setting': 3,\n",
       " 'flame': 11,\n",
       " 'upon': 7,\n",
       " 'sunk': 10,\n",
       " 'saw': 6,\n",
       " 'johnny': 1,\n",
       " 'marr': 1,\n",
       " 'primal': 1,\n",
       " 'hour': 9,\n",
       " 'sunday': 1,\n",
       " 'fully': 4,\n",
       " 'aware': 3,\n",
       " 'battle': 7,\n",
       " 'support': 6,\n",
       " 'fight': 5,\n",
       " 'today': 14,\n",
       " 'allegation': 1,\n",
       " 'timeline': 2,\n",
       " 'damn': 3,\n",
       " 'fast': 4,\n",
       " 'nw': 4,\n",
       " 'wth': 1,\n",
       " 'rotating': 1,\n",
       " 'w': 11,\n",
       " 'huge': 7,\n",
       " 'massive': 2,\n",
       " 'violent': 9,\n",
       " 'tornado': 3,\n",
       " 'great': 19,\n",
       " 'role': 7,\n",
       " 'never': 14,\n",
       " 'hog': 1,\n",
       " 'scene': 4,\n",
       " 'fellow': 2,\n",
       " 'cast': 4,\n",
       " 'plenty': 1,\n",
       " 'line': 7,\n",
       " 'comedy': 10,\n",
       " 'fell': 2,\n",
       " 'rock': 2,\n",
       " 'scraped': 1,\n",
       " 'butt': 1,\n",
       " 'nearly': 4,\n",
       " 'drowned': 6,\n",
       " 'summerk': 1,\n",
       " 'record': 2,\n",
       " 'hurricane': 8,\n",
       " 'drought': 12,\n",
       " 'blazing': 5,\n",
       " 'weatherstay': 1,\n",
       " 'std': 1,\n",
       " 'yet': 12,\n",
       " 'rejected': 1,\n",
       " 'slogan': 1,\n",
       " 'notification': 1,\n",
       " 'thats': 8,\n",
       " 'bad': 15,\n",
       " 'haha': 5,\n",
       " 'wouldve': 1,\n",
       " 'tho': 2,\n",
       " 'mudslide': 10,\n",
       " 'aw': 1,\n",
       " 'country': 10,\n",
       " 'latin': 1,\n",
       " 'america': 5,\n",
       " 'next': 8,\n",
       " 'argentina': 1,\n",
       " 'one': 49,\n",
       " 'week': 11,\n",
       " 'ago': 3,\n",
       " 'reported': 4,\n",
       " 'economic': 2,\n",
       " 'frequent': 1,\n",
       " 'thunder': 6,\n",
       " 'gusty': 1,\n",
       " 'part': 14,\n",
       " 'uptown': 1,\n",
       " 'midtown': 1,\n",
       " 'cn': 1,\n",
       " 'paramedic': 2,\n",
       " 'really': 18,\n",
       " 'leave': 5,\n",
       " 'someone': 7,\n",
       " 'inside': 9,\n",
       " 'building': 29,\n",
       " 'collapseblow': 1,\n",
       " 'halloikbenwill': 1,\n",
       " 'motivator': 1,\n",
       " 'hoot': 1,\n",
       " 'half': 12,\n",
       " 'see': 25,\n",
       " 'candidate': 1,\n",
       " 'giving': 6,\n",
       " 'cent': 1,\n",
       " 'stump': 1,\n",
       " 'speech': 1,\n",
       " 'destined': 1,\n",
       " 'st': 9,\n",
       " 'century': 2,\n",
       " 'new': 32,\n",
       " 'conan': 1,\n",
       " 'going': 26,\n",
       " 'splash': 1,\n",
       " 'greater': 2,\n",
       " 'arnold': 1,\n",
       " 'schwarzenegger': 1,\n",
       " 'jeanclaud': 1,\n",
       " 'van': 3,\n",
       " 'damme': 1,\n",
       " 'steven': 2,\n",
       " 'segal': 1,\n",
       " 'moron': 1,\n",
       " 'flag': 7,\n",
       " 'man': 16,\n",
       " 'brainless': 1,\n",
       " 'muscle': 2,\n",
       " 'dobut': 1,\n",
       " 'youre': 10,\n",
       " 'murderer': 9,\n",
       " 'village': 8,\n",
       " 'pugwash': 1,\n",
       " 'every': 13,\n",
       " 'truck': 9,\n",
       " 'town': 4,\n",
       " 'house': 21,\n",
       " 'population': 6,\n",
       " 'survive': 9,\n",
       " 'tonight': 8,\n",
       " 'wouldnt': 1,\n",
       " 'change': 12,\n",
       " 'thing': 12,\n",
       " 'starve': 1,\n",
       " 'death': 16,\n",
       " 'wild': 7,\n",
       " 'morel': 1,\n",
       " 'ambleside': 1,\n",
       " 'farmr': 1,\n",
       " 'martsunmushroom': 1,\n",
       " 'foragesecret': 1,\n",
       " 'know': 29,\n",
       " 'tree': 6,\n",
       " 'grow': 4,\n",
       " 'climbed': 1,\n",
       " 'wheelsio': 1,\n",
       " 'hawkhis': 1,\n",
       " 'knee': 2,\n",
       " 'injury': 21,\n",
       " 'wheres': 1,\n",
       " 'beltmr': 1,\n",
       " 'srk': 1,\n",
       " 'cook': 1,\n",
       " 'ur': 5,\n",
       " 'beautiful': 7,\n",
       " 'as': 13,\n",
       " 'punishment': 2,\n",
       " 'bombedout': 1,\n",
       " 'britain': 2,\n",
       " 'art': 7,\n",
       " 'undercover': 2,\n",
       " 'brother': 3,\n",
       " 'run': 11,\n",
       " 'steam': 2,\n",
       " 'find': 7,\n",
       " 'surprise': 4,\n",
       " 'amuse': 1,\n",
       " 'earring': 3,\n",
       " 'beach': 7,\n",
       " 'jewelry': 1,\n",
       " 'vacation': 1,\n",
       " 'keep': 10,\n",
       " 'calm': 2,\n",
       " 'flattened': 7,\n",
       " 'cold': 3,\n",
       " 'nuke': 2,\n",
       " 'ban': 5,\n",
       " 'ocean': 1,\n",
       " 'superiority': 1,\n",
       " 'unconditional': 1,\n",
       " 'surrender': 1,\n",
       " 'putin': 1,\n",
       " 'game': 9,\n",
       " 'set': 11,\n",
       " 'match': 3,\n",
       " 'release': 4,\n",
       " 'hostage': 15,\n",
       " 'wearing': 2,\n",
       " 'dead': 24,\n",
       " 'black': 15,\n",
       " 'flaming': 2,\n",
       " 'red': 6,\n",
       " 'stark': 2,\n",
       " 'white': 6,\n",
       " 'much': 26,\n",
       " 'esp': 1,\n",
       " 'debate': 1,\n",
       " 'go': 34,\n",
       " 'blue': 6,\n",
       " 'gold': 5,\n",
       " 'brown': 5,\n",
       " 'shoe': 3,\n",
       " 'result': 4,\n",
       " 'hahahah': 1,\n",
       " 'worrying': 1,\n",
       " 'performance': 6,\n",
       " 'incredible': 3,\n",
       " 'effected': 1,\n",
       " 'cali': 1,\n",
       " 'effectively': 1,\n",
       " 'teach': 1,\n",
       " 'kid': 6,\n",
       " 'danger': 6,\n",
       " 'drug': 4,\n",
       " 'project': 5,\n",
       " 'lrb': 3,\n",
       " 'unfortunately': 1,\n",
       " 'rrated': 1,\n",
       " 'rrb': 7,\n",
       " 'paid': 1,\n",
       " 'encaustic': 1,\n",
       " 'cerography': 1,\n",
       " 'portion': 2,\n",
       " 'till': 3,\n",
       " 'give': 13,\n",
       " 'voice': 2,\n",
       " 'deluge': 8,\n",
       " 'byityf': 1,\n",
       " 'hope': 8,\n",
       " 'ok': 8,\n",
       " 'warning': 15,\n",
       " 'dry': 2,\n",
       " 'thunderstorm': 11,\n",
       " 'bay': 2,\n",
       " 'weather': 9,\n",
       " 'cawx': 1,\n",
       " 'nwsbayarea': 1,\n",
       " 'enough': 10,\n",
       " 'held': 5,\n",
       " 'together': 7,\n",
       " 'skilled': 1,\n",
       " 'ensemble': 1,\n",
       " 'actor': 3,\n",
       " 'drama': 6,\n",
       " 'cube': 1,\n",
       " 'stepped': 2,\n",
       " 'broken': 3,\n",
       " 'glass': 5,\n",
       " 'pun': 2,\n",
       " 'tak': 1,\n",
       " 'sedar': 1,\n",
       " 'pain': 3,\n",
       " 'also': 9,\n",
       " 'bleeding': 9,\n",
       " 'shit': 14,\n",
       " 'progress': 2,\n",
       " 'shot': 11,\n",
       " 'syncopated': 1,\n",
       " 'style': 5,\n",
       " 'mimicking': 1,\n",
       " 'work': 30,\n",
       " 'subject': 3,\n",
       " 'pray': 3,\n",
       " 'turn': 8,\n",
       " 'idea': 11,\n",
       " 'documentary': 6,\n",
       " 'head': 10,\n",
       " 'rousing': 1,\n",
       " 'invigorating': 1,\n",
       " 'fun': 11,\n",
       " 'lacking': 1,\n",
       " 'mtv': 1,\n",
       " 'puffery': 1,\n",
       " 'say': 24,\n",
       " 'silas': 1,\n",
       " 'sliced': 1,\n",
       " 'headlinelike': 1,\n",
       " 'chopped': 1,\n",
       " 'piece': 5,\n",
       " 'cabbage': 1,\n",
       " 'gh': 1,\n",
       " 'twister': 7,\n",
       " 'found': 7,\n",
       " 'reunion': 6,\n",
       " 'island': 6,\n",
       " 'flight': 3,\n",
       " 'mh': 13,\n",
       " 'behind': 3,\n",
       " 'plane': 9,\n",
       " 'disappearance': 1,\n",
       " 'better': 9,\n",
       " 'mixed': 2,\n",
       " 'disapproval': 1,\n",
       " 'justine': 1,\n",
       " 'combined': 2,\n",
       " 'tinge': 1,\n",
       " 'understanding': 3,\n",
       " 'power': 11,\n",
       " 'jedi': 1,\n",
       " 'collection': 1,\n",
       " 'droid': 1,\n",
       " 'hasbro': 1,\n",
       " 'full': 24,\n",
       " 'read': 12,\n",
       " 'ebay': 8,\n",
       " 'effect': 13,\n",
       " 'hiroshima': 15,\n",
       " 'nagasaki': 3,\n",
       " 'bombing': 13,\n",
       " 'felt': 2,\n",
       " 'fan': 11,\n",
       " 'angry': 3,\n",
       " 'odeon': 2,\n",
       " 'cinema': 8,\n",
       " 'evacuated': 10,\n",
       " 'following': 4,\n",
       " 'false': 5,\n",
       " 'alarm': 4,\n",
       " 'doesnt': 8,\n",
       " 'replace': 5,\n",
       " 'eyewitness': 6,\n",
       " 'video': 23,\n",
       " 'ferguson': 2,\n",
       " 'wounded': 9,\n",
       " 'suspect': 7,\n",
       " 'exchanging': 3,\n",
       " 'richmond': 3,\n",
       " 'police': 18,\n",
       " 'officer': 6,\n",
       " 'exchange': 1,\n",
       " 'gunfire': 1,\n",
       " 'incarnation': 1,\n",
       " 'fizz': 1,\n",
       " 'infectious': 2,\n",
       " 'stop': 10,\n",
       " 'terrorism': 7,\n",
       " 'inviting': 1,\n",
       " 'arsonist': 3,\n",
       " 'join': 2,\n",
       " 'brigade': 1,\n",
       " 'telegraph': 1,\n",
       " 'happen': 2,\n",
       " 'anywhere': 2,\n",
       " 'school': 14,\n",
       " 'etc': 2,\n",
       " 'learn': 7,\n",
       " 'trauma': 6,\n",
       " 'parent': 3,\n",
       " '\\x89Ã»': 4,\n",
       " 'u': 53,\n",
       " 'trip': 4,\n",
       " 'n': 7,\n",
       " 'fall': 13,\n",
       " 'cliff': 6,\n",
       " 'tweet': 5,\n",
       " 'loses': 3,\n",
       " 'bite': 1,\n",
       " 'lastminute': 1,\n",
       " 'happy': 6,\n",
       " 'ending': 3,\n",
       " 'le': 3,\n",
       " 'plausible': 1,\n",
       " 'rest': 4,\n",
       " 'picture': 11,\n",
       " 'owner': 2,\n",
       " 'charged': 12,\n",
       " 'swear': 1,\n",
       " 'secret': 7,\n",
       " 'well': 15,\n",
       " 'uncover': 1,\n",
       " 'god': 9,\n",
       " 'slumber': 1,\n",
       " 'there': 5,\n",
       " 'blight': 4,\n",
       " 'trying': 4,\n",
       " 'racist': 1,\n",
       " 'elitist': 1,\n",
       " 'almost': 12,\n",
       " 'spooky': 1,\n",
       " 'sulky': 1,\n",
       " 'calculating': 1,\n",
       " 'lolita': 1,\n",
       " 'catch': 4,\n",
       " 'finally': 7,\n",
       " 'monwabisi': 1,\n",
       " 'lol': 16,\n",
       " 'hlongwane': 1,\n",
       " 'ryt': 1,\n",
       " 'twin': 2,\n",
       " 'r': 11,\n",
       " 'destroy': 11,\n",
       " 'ashestoashes': 1,\n",
       " 'dad': 2,\n",
       " 'survived': 8,\n",
       " 'driving': 5,\n",
       " 'missing': 4,\n",
       " 'migrant': 5,\n",
       " 'med': 1,\n",
       " 'rescuer': 8,\n",
       " 'search': 4,\n",
       " 'survivor': 9,\n",
       " 'boat': 7,\n",
       " 'carrying': 2,\n",
       " 'many': 25,\n",
       " 'migrants\\x89Ã»': 1,\n",
       " 'tom': 1,\n",
       " 'clancy': 1,\n",
       " 'military': 7,\n",
       " 'paperback': 1,\n",
       " 'tomclancy': 1,\n",
       " 'anyone': 4,\n",
       " 'answer': 2,\n",
       " 'need': 18,\n",
       " 'contact': 3,\n",
       " 'flooding': 6,\n",
       " 'vietnam': 2,\n",
       " 'situation': 2,\n",
       " 'night': 7,\n",
       " 'panic': 11,\n",
       " 'gem': 6,\n",
       " 'obsession': 1,\n",
       " 'hijacking': 5,\n",
       " 'computer': 8,\n",
       " 'send': 8,\n",
       " 'data': 7,\n",
       " 'wave': 9,\n",
       " 'hat': 6,\n",
       " 'prebreak': 10,\n",
       " 'health': 3,\n",
       " 'care': 6,\n",
       " 'review': 5,\n",
       " 'investigative': 1,\n",
       " 'journalism': 1,\n",
       " 'sick': 3,\n",
       " 'injured': 11,\n",
       " 'patient': 2,\n",
       " 'local': 2,\n",
       " 'er': 3,\n",
       " 'entertainment': 3,\n",
       " 'derives': 1,\n",
       " 'sticking': 1,\n",
       " 'fact': 4,\n",
       " 'live': 13,\n",
       " 'noaa': 1,\n",
       " 'tracking': 1,\n",
       " 'looping': 1,\n",
       " 'wedaugth': 1,\n",
       " 'electrocuted': 6,\n",
       " 'morning': 5,\n",
       " 'becomes': 4,\n",
       " 'race': 3,\n",
       " 'education': 2,\n",
       " 'catastrophe': 9,\n",
       " 'inundated': 6,\n",
       " 'soggy': 1,\n",
       " 'bottom': 2,\n",
       " 'lashing': 1,\n",
       " 'moist': 1,\n",
       " 'rioting': 8,\n",
       " 'valley': 2,\n",
       " 'penn': 1,\n",
       " 'storyline': 1,\n",
       " 'interesting': 4,\n",
       " 'entertaining': 4,\n",
       " 'nt': 15,\n",
       " 'magical': 1,\n",
       " 'quality': 4,\n",
       " 'beginning': 3,\n",
       " 'later': 4,\n",
       " 'crime': 6,\n",
       " 'put': 9,\n",
       " 'moscow': 1,\n",
       " 'director': 2,\n",
       " 'bourne': 1,\n",
       " 'directs': 1,\n",
       " 'traffic': 6,\n",
       " 'nice': 3,\n",
       " 'wintry': 1,\n",
       " 'location': 2,\n",
       " 'absorbs': 1,\n",
       " 'spycraft': 1,\n",
       " 'us': 2,\n",
       " 'damon': 1,\n",
       " 'ability': 2,\n",
       " 'focused': 1,\n",
       " 'sincere': 1,\n",
       " 'surface': 2,\n",
       " 'loversontherun': 1,\n",
       " 'flick': 5,\n",
       " 'lot': 10,\n",
       " 'common': 2,\n",
       " 'piesiewicz': 1,\n",
       " 'kieslowski': 1,\n",
       " 'earlier': 4,\n",
       " 'double': 5,\n",
       " 'veronique': 1,\n",
       " 'soudelors': 2,\n",
       " 'predicted': 3,\n",
       " 'path': 3,\n",
       " 'approach': 2,\n",
       " 'taiwan': 7,\n",
       " 'expected': 5,\n",
       " 'landfall': 2,\n",
       " 'southern': 3,\n",
       " 'china': 4,\n",
       " 's\\x89Ã»': 2,\n",
       " 'north': 4,\n",
       " 'japton': 2,\n",
       " 'large': 6,\n",
       " 'po': 4,\n",
       " 'arwx': 2,\n",
       " 'respond': 2,\n",
       " 'chemical': 8,\n",
       " 'spill': 14,\n",
       " 'downtown': 5,\n",
       " 'beaumont': 1,\n",
       " 'benews': 1,\n",
       " 'ready': 6,\n",
       " 'client': 3,\n",
       " 'outage': 2,\n",
       " 'vet': 3,\n",
       " 'design': 4,\n",
       " 'western': 3,\n",
       " 'food': 3,\n",
       " 'en': 1,\n",
       " 'masse': 1,\n",
       " 'causing': 4,\n",
       " 'public': 3,\n",
       " 'backlash': 1,\n",
       " 'san': 2,\n",
       " 'antonio': 1,\n",
       " 'star': 4,\n",
       " 'coach': 3,\n",
       " 'dan': 2,\n",
       " 'hughes': 1,\n",
       " 'sideline': 4,\n",
       " 'chair': 2,\n",
       " 'onto': 5,\n",
       " 'floor': 2,\n",
       " 'stretcher': 4,\n",
       " 'except': 4,\n",
       " 'actually': 5,\n",
       " 'colorado': 4,\n",
       " 'tomorrow': 10,\n",
       " 'dreaming': 1,\n",
       " 'capture': 7,\n",
       " 'complexity': 1,\n",
       " 'trial': 3,\n",
       " 'tribulation': 1,\n",
       " 'gone': 4,\n",
       " 'warped': 1,\n",
       " 'tony': 1,\n",
       " 'played': 6,\n",
       " 'issue': 6,\n",
       " 'showed': 3,\n",
       " 'sleeping': 1,\n",
       " 'siren': 6,\n",
       " 'attila': 1,\n",
       " 'hasnt': 2,\n",
       " 'coat': 1,\n",
       " 'hand': 5,\n",
       " 'id': 8,\n",
       " 'worn': 1,\n",
       " 'certainty': 1,\n",
       " 'armageddon': 7,\n",
       " 'bear': 2,\n",
       " 'occasion': 2,\n",
       " 'electrocute': 3,\n",
       " 'thanks': 5,\n",
       " 'normal': 3,\n",
       " 'sit': 4,\n",
       " 'front': 6,\n",
       " 'uber': 1,\n",
       " 'driver': 4,\n",
       " 'original': 5,\n",
       " 'sensei': 1,\n",
       " 'write': 1,\n",
       " 'rhyme': 1,\n",
       " 'attic': 1,\n",
       " 'reviewing': 1,\n",
       " 'policy': 8,\n",
       " 'leaving': 3,\n",
       " 'hundred': 4,\n",
       " 'commuter': 1,\n",
       " 'stranded': 2,\n",
       " 'hail\\x89Ã»': 1,\n",
       " 'patio': 1,\n",
       " 'table': 1,\n",
       " 'umbrella': 1,\n",
       " 'flipped': 1,\n",
       " 'foul': 2,\n",
       " 'play': 8,\n",
       " 'instead': 6,\n",
       " 'suspense': 2,\n",
       " 'writer': 2,\n",
       " 'divided': 2,\n",
       " 'headed': 2,\n",
       " 'destruction': 6,\n",
       " 'stand': 5,\n",
       " 'matthew': 2,\n",
       " 'anthropologically': 1,\n",
       " 'detailed': 1,\n",
       " 'realization': 1,\n",
       " 'early': 5,\n",
       " 'suburbia': 1,\n",
       " 'significant': 1,\n",
       " 'overstated': 1,\n",
       " 'playful': 3,\n",
       " 'constantly': 1,\n",
       " 'frustrates': 1,\n",
       " 'desire': 5,\n",
       " 'truth': 1,\n",
       " 'deconstructing': 1,\n",
       " 'format': 1,\n",
       " 'biography': 1,\n",
       " 'manner': 1,\n",
       " 'derrida': 3,\n",
       " 'doubtless': 1,\n",
       " 'blessing': 2,\n",
       " 'close': 5,\n",
       " 'floyd': 1,\n",
       " 'mayweathers': 1,\n",
       " 'money': 8,\n",
       " 'bloody': 10,\n",
       " 'elbow': 1,\n",
       " 'boxing': 1,\n",
       " 'kill': 11,\n",
       " 'general': 4,\n",
       " 'highestranking': 1,\n",
       " 'fatality': 10,\n",
       " 'wicked': 2,\n",
       " 'wont': 8,\n",
       " 'fbi': 1,\n",
       " 'stole': 1,\n",
       " 'married': 2,\n",
       " 'honduran': 1,\n",
       " 'minor': 2,\n",
       " 'sex': 2,\n",
       " 'latifah': 1,\n",
       " 'offer': 2,\n",
       " 'seemed': 3,\n",
       " 'flaunting': 1,\n",
       " 'gift': 1,\n",
       " 'sexy': 1,\n",
       " 'happiest': 1,\n",
       " 'deal': 7,\n",
       " 'surprising': 1,\n",
       " 'infuriating': 1,\n",
       " 'flaw': 1,\n",
       " 'least': 8,\n",
       " 'amy': 2,\n",
       " 'selfabsorbed': 2,\n",
       " 'personality': 1,\n",
       " 'honesty': 4,\n",
       " 'taking': 6,\n",
       " 'extra': 1,\n",
       " 'security': 6,\n",
       " 'harrybecareful': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word2count = {}\n",
    "for data in ptext_cleaned:\n",
    "    words = data \n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            \n",
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count : \u001b[34m5830\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count :\" ,colored(len(word2count),'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting best 100 features only\n",
    "import heapq \n",
    "ptext_words = heapq.nlargest(1200, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.em;color:#0eea3a\">Vectorization\n",
    "    \n",
    "    *Arbitarary words (units/tokens) into fixed length of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Converting sentences into vectors: Xtrain data\n",
    "x_final = []\n",
    "for data in ptext_cleaned:\n",
    "    vec = []\n",
    "    for word in ptext_words:\n",
    "        if word in data: \n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    x_final.append(vec)\n",
    "    \n",
    "X_final = np.array(x_final)\n",
    "print(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
