{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0JTFDEojBou"
   },
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#c37235\">Bag of words, Tokenizer, and Doc2Vec\n",
    "<font color=darkblue>\n",
    "    \n",
    "    \n",
    "- This is simple approach of bag of Words, Tokenizer, and Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E7do5pgZjPM6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayanthikishore/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Tenorflow \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Tenorflow Padding Sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['there is a snake in my boot', 'there is boot snake in my house', 'a a a a']\n",
    "labels = [75, 12, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['a', 'a', 'a', 'a']),\n",
       "        list(['there', 'is', 'a', 'snake', 'in', 'my', 'boot']),\n",
       "        list(['there', 'is', 'boot', 'snake', 'in', 'my', 'house'])],\n",
       "       dtype=object),\n",
       " [['there', 'is', 'a', 'snake', 'in', 'my', 'boot'],\n",
       "  ['there', 'is', 'boot', 'snake', 'in', 'my', 'house'],\n",
       "  ['a', 'a', 'a', 'a']])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [tweet.split(' ') for tweet in tweets]\n",
    "unique_words = np.unique(tweets)\n",
    "unique_words, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'there': 0,\n",
       " 'is': 1,\n",
       " 'a': 2,\n",
       " 'snake': 3,\n",
       " 'in': 4,\n",
       " 'my': 5,\n",
       " 'boot': 6,\n",
       " 'house': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = {}\n",
    "\n",
    "counter = 0\n",
    "for tweet in tweets:\n",
    "  for word in tweet:\n",
    "    if word not in tokenizer:\n",
    "      tokenizer[word] = counter\n",
    "      counter += 1\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#6bc335\">Bag of words \n",
    "<font color=darkblue>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 0, 1, 1, 1, 1, 1], [0, 0, 4, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words = []\n",
    "# count_count = [0]*len(unique_words)\n",
    "\n",
    "for tweet in tweets:\n",
    "  word_count = [0]*(counter)\n",
    "  \n",
    "  #Counts instence of every unique word that appears\n",
    "  for word in tweet:\n",
    "    locWord = tokenizer[word] # Get the index location of the words\n",
    "    word_count[locWord] += 1 # Counts the number of times that word appears\n",
    "    \n",
    "  # Append after finnished counting\n",
    "  bag_words.append(word_count)\n",
    "\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 4, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mUuAl3edkn4N"
   },
   "outputs": [],
   "source": [
    "# Twitter length\n",
    "token_len = 50\n",
    "\n",
    "# Create Decorator\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer() # Sets up the Tikenizer which we will feed\n",
    "\n",
    "# Fitting the Tokenizer and building our Corpus\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Create our sequence\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "\n",
    "# Padding the text\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=token_len, padding= 'post', truncating='post')\n",
    "\n",
    "# Convert array to Tensor\n",
    "X = tf.constant(X, dtype=tf.int64)\n",
    "y = tf.constant(labels, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "EaG_Mib2ov60",
    "outputId": "1b861bf3-0155-485a-b52a-1bf53f77377b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JLtnjn6RvBJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  <span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#353ac3\">Doc2Vector \n",
    "<font color=darkblue>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['data', 'scientists', 'should', 'know', 'the', 'mathematics', ',', 'statistics', ',', 'and', 'programming'], tags=['0']),\n",
       " TaggedDocument(words=['data', 'scientist', 'familiar', 'in', 'machine', 'learning', 'deep', 'learning', 'and', 'artificial', 'intelligence'], tags=['1']),\n",
       " TaggedDocument(words=['data', 'scientist', 'should', 'know', 'python', 'and', 'r', 'coding'], tags=['2']),\n",
       " TaggedDocument(words=['data', 'scientist', 'should', 'know', 'the', 'how', 'to', 'train', 'test', 'and', 'validation', 'of', 'the', 'model'], tags=['3']),\n",
       " TaggedDocument(words=['metrics', 'is', 'more', 'important', 'for', 'model', 'validation'], tags=['4'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledata = [\"Data scientists should know the Mathematics, statistics, and programming\",\n",
    "             \"Data scientist familiar in machine learning deep learning and artificial intelligence\",\n",
    "             \"Data Scientist should know python and R coding\",\n",
    "             \"Data Scientist should know the how to train test and validation of the model\",\n",
    "              \"Metrics is more important for model validation\"]\n",
    "\n",
    "tag_sample = [TaggedDocument(words = word_tokenize(dat.lower()), tags =[str(i)]) for i,dat in enumerate(sampledata)]\n",
    "tag_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# # nltk.download()\n",
    "# from gensim.models.doc2vec import Doc2Vec, Tagdoc\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# data = [\"I love machine learning. Its awesome.\",\n",
    "#         \"I love coding in python\",\n",
    "#         \"I love building chatbots\",\n",
    "#         \"they chat amagingly well\"]\n",
    "\n",
    "# tagged_data = [Tagdoc(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "# tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n",
      "Iteration number 1\n",
      "Iteration number 2\n",
      "Iteration number 3\n",
      "Iteration number 4\n",
      "Iteration number 5\n",
      "Iteration number 6\n",
      "Iteration number 7\n",
      "Iteration number 8\n",
      "Iteration number 9\n",
      "Iteration number 10\n",
      "Iteration number 11\n",
      "Iteration number 12\n",
      "Iteration number 13\n",
      "Iteration number 14\n",
      "Iteration number 15\n",
      "Iteration number 16\n",
      "Iteration number 17\n",
      "Iteration number 18\n",
      "Iteration number 19\n",
      "Iteration number 20\n",
      "Iteration number 21\n",
      "Iteration number 22\n",
      "Iteration number 23\n",
      "Iteration number 24\n",
      "Iteration number 25\n",
      "Iteration number 26\n",
      "Iteration number 27\n",
      "Iteration number 28\n",
      "Iteration number 29\n",
      "Iteration number 30\n",
      "Iteration number 31\n",
      "Iteration number 32\n",
      "Iteration number 33\n",
      "Iteration number 34\n",
      "Iteration number 35\n",
      "Iteration number 36\n",
      "Iteration number 37\n",
      "Iteration number 38\n",
      "Iteration number 39\n",
      "Iteration number 40\n",
      "Iteration number 41\n",
      "Iteration number 42\n",
      "Iteration number 43\n",
      "Iteration number 44\n",
      "Iteration number 45\n",
      "Iteration number 46\n",
      "Iteration number 47\n",
      "Iteration number 48\n",
      "Iteration number 49\n",
      "Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "doc2vecmodel = Doc2Vec(dm=1, vector_size=20, alpha=0.025,negative=5, hs=0,min_count=1,min_alpha=0.00025,epochs=50)\n",
    "  \n",
    "doc2vecmodel.build_vocab(tag_sample)\n",
    "\n",
    "for epoch in range(doc2vecmodel.epochs):\n",
    "    print(\"Iteration number {0}\".format(epoch))\n",
    "    doc2vecmodel.train(tag_sample,total_examples=doc2vecmodel.corpus_count,\n",
    "                      epochs=doc2vecmodel.epochs)\n",
    "    #decrease the learning rate\n",
    "    doc2vecmodel.alpha -=0.0002\n",
    "    #fix the learning rate, no decay\n",
    "    doc2vecmodel.min_alpha=doc2vecmodel.alpha\n",
    "    \n",
    "#save the model\n",
    "doc2vecmodel.save(\"/home/jayanthikishore/Downloads/doc2vec.model\")\n",
    "print(\"Model Successfully Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d20,n5,w5,s0.001,t3)\n"
     ]
    }
   ],
   "source": [
    "print(doc2vecmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer:  [ 0.00600685 -0.01081637 -0.02342382  0.01165458  0.00160417 -0.01647668\n",
      " -0.00257421  0.00148563 -0.02041394  0.00884137 -0.01830285 -0.02257978\n",
      " -0.01210968  0.00044988  0.02139286 -0.0040398  -0.0010805   0.02480741\n",
      " -0.00246424 -0.01426513]\n",
      "[('3', 0.22874553501605988), ('2', 0.1877303570508957), ('0', 0.013264761306345463)]\n",
      "[-0.85137886  0.89662504 -1.123845    0.37472183  1.1933655  -1.7735094\n",
      " -1.6574339  -2.1300223   1.4840639  -1.8634331   1.7724966   1.2565795\n",
      " -0.77021134 -0.4470133   0.07611626  0.38757312 -0.08701865 -2.3551352\n",
      " -0.5461885   0.17398375]\n"
     ]
    }
   ],
   "source": [
    "#Access the saved model file\n",
    "doc2vecmodel= Doc2Vec.load(\"/home/jayanthikishore/Downloads/d2v.model\")\n",
    "\n",
    "#to find the vector of a document which is not in the training data\n",
    "test_line = word_tokenize(\"Data Scientist calculates metrics for every model\")\n",
    "vec = doc2vecmodel.infer_vector(test_line)\n",
    "print(\"Infer: \",vec)\n",
    "\n",
    "#to find most similar doc using tags\n",
    "sim_doc = doc2vecmodel.docvecs.most_similar(\"1\")\n",
    "print(sim_doc)\n",
    "\n",
    "# otherwise\n",
    "print(doc2vecmodel.docvecs[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bag-O-Words.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
